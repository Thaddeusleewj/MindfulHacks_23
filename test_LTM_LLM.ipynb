{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load .env variables\n",
    "from dotenv import load_dotenv\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "# Cassandra), and OpenAI (to generate embeddings)\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# These are used to authenticate with Astra DB\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "# Support for dataset retrieval with Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "import json\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore Long term memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_size = 1536 # Dimensions of the OpenAIEmbeddings\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings().embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In actual usage, you would set `k` to be a higher value, but we use k=1 to show that\n",
    "# the vector lookup still returns the semantically relevant information\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# When added to an agent, the memory object can save pertinent information from conversations or used tools\n",
    "memory.save_context({\"input\": \"My favorite food is pizza\"}, {\"output\": \"that's good to know\"})\n",
    "memory.save_context({\"input\": \"My favorite sport is soccer\"}, {\"output\": \"...\"})\n",
    "memory.save_context({\"input\": \"I don't the Celtics\"}, {\"output\": \"ok\"}) #\n",
    "\n",
    "memory.sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Relevant pieces of previous conversation:\n",
      "input: My favorite food is pizza\n",
      "output: that's good to know\n",
      "\n",
      "(You do not need to use these pieces of information if not relevant)\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Perry, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Perry, I'm doing great. How about you?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0) # Can be any valid LLM\n",
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Relevant pieces of previous conversation:\n",
    "{history}\n",
    "\n",
    "(You do not need to use these pieces of information if not relevant)\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
    ")\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    prompt=PROMPT,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, my name is Perry, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Relevant pieces of previous conversation:\n",
      "input: My favorite sport is soccer\n",
      "output: ...\n",
      "\n",
      "(You do not need to use these pieces of information if not relevant)\n",
      "\n",
      "Current conversation:\n",
      "Human: what's my favorite sport?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You told me earlier that your favorite sport is soccer.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, the basketball related content is surfaced\n",
    "conversation_with_summary.predict(input=\"what's my favorite sport?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from Supabase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 05:53:52,871:INFO - HTTP Request: GET https://xateluqtqxktrcaeqmlz.supabase.co/rest/v1/therapy?select=%2A&order=created_at.desc \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_points_dict:  {'MainProblems': 'Broke up with girlfriend, feeling devastated and numb, struggling with self-blame', 'AnythingRelevant': 'Feeling depressed, having difficulty with appetite and sleep, finding it hard to engage in hobbies'}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error: checkUpChain Failed -> Could not parse function call data: Expecting property name enclosed in double quotes: line 4 column 1 (char 270)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:52\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mloads(function_call_info, strict\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict)\n\u001b[0;32m     53\u001b[0m \u001b[39mexcept\u001b[39;00m (json\u001b[39m.\u001b[39mJSONDecodeError, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    358\u001b[0m     kw[\u001b[39m'\u001b[39m\u001b[39mparse_constant\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m parse_constant\n\u001b[1;32m--> 359\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mdecode(s)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 4 column 1 (char 270)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\llm\\main.py:154\u001b[0m, in \u001b[0;36mTherapistLLM._checkUpChain\u001b[1;34m(self, MainProblems, AnythingRelevant)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckUpChain\u001b[39m.\u001b[39;49mrun(MainProblems \u001b[39m=\u001b[39;49m MainProblems,AnythingRelevant \u001b[39m=\u001b[39;49m AnythingRelevant)\n\u001b[0;32m    156\u001b[0m     \u001b[39m# Validate the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\chains\\base.py:492\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m--> 492\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[0;32m    493\u001b[0m         _output_key\n\u001b[0;32m    494\u001b[0m     ]\n\u001b[0;32m    496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\chains\\base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\chains\\base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    289\u001b[0m     )\n\u001b[0;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\chains\\llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m     91\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate([inputs], run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m---> 92\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\chains\\llm.py:220\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[1;34m(self, llm_result)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m result \u001b[39m=\u001b[39m [\n\u001b[0;32m    221\u001b[0m     \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     {\n\u001b[0;32m    223\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_parser\u001b[39m.\u001b[39mparse_result(generation),\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfull_generation\u001b[39m\u001b[39m\"\u001b[39m: generation,\n\u001b[0;32m    225\u001b[0m     }\n\u001b[0;32m    226\u001b[0m     \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m llm_result\u001b[39m.\u001b[39mgenerations\n\u001b[0;32m    227\u001b[0m ]\n\u001b[0;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_final_only:\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\chains\\llm.py:223\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m result \u001b[39m=\u001b[39m [\n\u001b[0;32m    221\u001b[0m     \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     {\n\u001b[1;32m--> 223\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse_result(generation),\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfull_generation\u001b[39m\u001b[39m\"\u001b[39m: generation,\n\u001b[0;32m    225\u001b[0m     }\n\u001b[0;32m    226\u001b[0m     \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m llm_result\u001b[39m.\u001b[39mgenerations\n\u001b[0;32m    227\u001b[0m ]\n\u001b[0;32m    228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_final_only:\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\env\\lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:54\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[39mexcept\u001b[39;00m (json\u001b[39m.\u001b[39mJSONDecodeError, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m---> 54\u001b[0m         \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     55\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse function call data: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     57\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Could not parse function call data: Expecting property name enclosed in double quotes: line 4 column 1 (char 270)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\test_LTM_LLM.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stu-kalebnim/Documents/GitHub/MindfulHacks_23/test_LTM_LLM.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m SUPABASE_KEY\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InhhdGVsdXF0cXhrdHJjYWVxbWx6Iiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTY2Njc1MzIsImV4cCI6MjAxMjI0MzUzMn0.fKeq_LSYQxM21EY2o5mCxbg9rA9rFqvBuyexXuOpQMo\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stu-kalebnim/Documents/GitHub/MindfulHacks_23/test_LTM_LLM.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m therapistLLM \u001b[39m=\u001b[39m TherapistLLM(SUPABASE_URL\u001b[39m=\u001b[39mSUPABASE_URL, SUPABASE_KEY\u001b[39m=\u001b[39mSUPABASE_KEY)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/stu-kalebnim/Documents/GitHub/MindfulHacks_23/test_LTM_LLM.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m checkup_question \u001b[39m=\u001b[39m therapistLLM\u001b[39m.\u001b[39;49mget_checkUp_question()\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\llm\\main.py:197\u001b[0m, in \u001b[0;36mget_checkUp_question\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m key_points_dict = self._transcriptExtractorChain(latest_transcript = latest_transcript)\n\u001b[0;32m    195\u001b[0m print('key_points_dict: ', key_points_dict)\n\u001b[1;32m--> 197\u001b[0m # Run the checkup chain\n\u001b[0;32m    198\u001b[0m checkUp_questions = self._checkUpChain(MainProblems = key_points_dict[\"MainProblems\"],AnythingRelevant = key_points_dict[\"AnythingRelevant\"])\n\u001b[0;32m    199\u001b[0m print('checkUp_questions: ', checkUp_questions)\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\llm\\main.py:165\u001b[0m, in \u001b[0;36mTherapistLLM._checkUpChain\u001b[1;34m(self, MainProblems, AnythingRelevant)\u001b[0m\n\u001b[0;32m    162\u001b[0m         results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckUpChainGPT4\u001b[39m.\u001b[39mrun(MainProblems \u001b[39m=\u001b[39m MainProblems,AnythingRelevant \u001b[39m=\u001b[39m AnythingRelevant, feedback \u001b[39m=\u001b[39m validation_results[\u001b[39m1\u001b[39m])\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 165\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: checkUpChain Failed -> \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "\u001b[1;31mException\u001b[0m: Error: checkUpChain Failed -> Could not parse function call data: Expecting property name enclosed in double quotes: line 4 column 1 (char 270)"
     ]
    }
   ],
   "source": [
    "from llm.main import TherapistLLM\n",
    "# Supabase\n",
    "SUPABASE_URL=\"https://xateluqtqxktrcaeqmlz.supabase.co\"\n",
    "SUPABASE_KEY=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InhhdGVsdXF0cXhrdHJjYWVxbWx6Iiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTY2Njc1MzIsImV4cCI6MjAxMjI0MzUzMn0.fKeq_LSYQxM21EY2o5mCxbg9rA9rFqvBuyexXuOpQMo\"\n",
    "\n",
    "therapistLLM = TherapistLLM(SUPABASE_URL=SUPABASE_URL, SUPABASE_KEY=SUPABASE_KEY)\n",
    "checkup_question = therapistLLM.get_checkUp_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "follow_up_checkUp_advice:  {'Advice1': \"One of the first steps in overcoming the challenges and emotions related to your recent breakup is to allow yourself to feel and express your emotions. It's important to acknowledge that breakup pain and sadness are real, and it's okay to let it out. Continue to use your journal as a safe space to vent and process your feelings. Write down your emotions and experiences, and use this journal as your go-to friend to confide in. This practice can bring a big sense of relief and help you on your journey of healing.\"}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Advice2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\test_LTM_LLM.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/stu-kalebnim/Documents/GitHub/MindfulHacks_23/test_LTM_LLM.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m followUpAdvice \u001b[39m=\u001b[39m therapistLLM\u001b[39m.\u001b[39;49mget_follow_up_checkUp_advice(user_response\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mOne of the first things I\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mve done is just let myself feel it all. You know, it\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms important to admit that breakup pain and sadness are real, and it\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms okay to let it out. I\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mve shed some tears, maybe screamed into a pillow a couple of times, and even scribbled down my feelings in this journal. It\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms like this journal has become my go-to friend to vent to. Getting all those emotions out has honestly been a big relief.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/stu-kalebnim/Documents/GitHub/MindfulHacks_23/test_LTM_LLM.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m followUpAdvice\n",
      "File \u001b[1;32mc:\\Users\\stu-kalebnim\\Documents\\GitHub\\MindfulHacks_23\\llm\\main.py:221\u001b[0m, in \u001b[0;36mTherapistLLM.get_follow_up_checkUp_advice\u001b[1;34m(self, user_response)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfollow_up_checkUp_advice: \u001b[39m\u001b[39m'\u001b[39m, follow_up_checkUp_advice)\n\u001b[0;32m    220\u001b[0m \u001b[39m# Update the latest journal prompt\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdateLatestJournalPrompt(JournalPrompt1 \u001b[39m=\u001b[39m follow_up_checkUp_advice[\u001b[39m\"\u001b[39m\u001b[39mAdvice1\u001b[39m\u001b[39m\"\u001b[39m],JournalPrompt2 \u001b[39m=\u001b[39m follow_up_checkUp_advice[\u001b[39m\"\u001b[39;49m\u001b[39mAdvice2\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    224\u001b[0m \u001b[39m# TODO \u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m#(f\"This is the follow up check up Advice1 based on :checkup{checkUp_question}\", f\"This is follow up check up Advice2 and user_response-->{user_response}\")\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[39mreturn\u001b[39;00m follow_up_checkUp_advice\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Advice2'"
     ]
    }
   ],
   "source": [
    "followUpAdvice = therapistLLM.get_follow_up_checkUp_advice(user_response= \"One of the first things I've done is just let myself feel it all. You know, it's important to admit that breakup pain and sadness are real, and it's okay to let it out. I've shed some tears, maybe screamed into a pillow a couple of times, and even scribbled down my feelings in this journal. It's like this journal has become my go-to friend to vent to. Getting all those emotions out has honestly been a big relief.\")\n",
    "followUpAdvice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and a theripist. The theripist is meant to help the human with her mental health issues in a positive manner, giving inspirational advice with relevance to the context of the human. If the theripist does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Relevant pieces of previous conversation:\n",
      "input: I'm feeling sad today,my exams are coming up and I'm not prepared at all\n",
      "response:  I understand how you feel. It can be really overwhelming when you have a lot of exams coming up and you don't feel prepared. Have you been studying for them? What can I do to help you?\n",
      "input: I'm feeling sad today,my exams are coming up and I'm not prepared at all\n",
      "output:  I understand how you feel. It can be really overwhelming when you have a lot of exams coming up and you don't feel prepared. Have you been studying for them? What can I do to help you?\n",
      "input: My problem is I very sad due to a breakup with my girlfriend\n",
      "output: Oh, I'm so sorry to hear that, that can be really tough. Want to talk about it? I’m happy to lend an ear. Was it a mutual decision or a difficult break up?\n",
      "\n",
      "(You do not need to use these pieces of information if not relevant)\n",
      "\n",
      "Current conversation:\n",
      "Human: yeah, I've been studying but i cant perform due to my situation at home, my parents keep arguging\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry to hear that. It can be really difficult to focus on studying when there is a lot of tension at home. Is there anything I can do to help you? Do you have any support system you can rely on?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "therapistLLM.chat(\"yeah, I've been studying but i cant perform due to my situation at home, my parents keep arguging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and a theripist. The theripist is meant to help the human with her mental health issues in a positive manner, giving inspirational advice with relevance to the context of the human. If the theripist does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Relevant pieces of previous conversation:\n",
      "input: I'm feeling sad today,my exams are coming up and I'm not prepared at all\n",
      "response:  I understand how you feel. It can be really overwhelming when you have a lot of exams coming up and you don't feel prepared. Have you been studying for them? What can I do to help you?\n",
      "input: My problem is I very sad due to a breakup with my girlfriend\n",
      "output: Oh, I'm so sorry to hear that, that can be really tough. Want to talk about it? I’m happy to lend an ear. Was it a mutual decision or a difficult break up?\n",
      "input: My problem is I very sad due to a breakup with my girlfriend\n",
      "output: Oh, I'm so sorry to hear that, that can be really tough. Want to talk about it? I’m happy to lend an ear. Was it a mutual decision or a difficult break up?\n",
      "\n",
      "(You do not need to use these pieces of information if not relevant)\n",
      "\n",
      "Current conversation:\n",
      "Human: Just talk to me, give me advice on what to due\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Sure, I'm here to listen and offer advice. What do you think would be the best way to move forward from this breakup? Have you thought about talking to a friend or family member about it? It can be really helpful to have someone to talk to about these things.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "therapistLLM.chat(\"Just talk to me, give me advice on what to due\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetrieverMemory(retriever=VectorStoreRetriever(tags=['FAISS'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x000001A402B03B20>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "therapistLLM.memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
